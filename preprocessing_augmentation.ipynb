{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import *\n",
    "#from model import *\n",
    "import rasterio as rio \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from utils import *\n",
    "import torch \n",
    "import icecream \n",
    "import tqdm\n",
    "import PIL\n",
    "%matplotlib inline\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observe the full images\n",
    "We load train and test data, rebuild the full images using the merge_raster.py file and save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"glaciers_mapping_downsampled\"\n",
    "data_paths = {}\n",
    "\n",
    "#build paths for each pipeline\n",
    "for pipeline in [\"train\", \"test\"]:\n",
    "    for date in [0,1]:\n",
    "        data_paths[f\"{pipeline}_date{date}\"] = f\"{file_path}/{pipeline}/date{date}\"\n",
    "    data_paths[f\"{pipeline}_gt\"] = f\"{file_path}/{pipeline}/gt\"\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reconstruct full images\n",
    "for key, path in data_paths.items():\n",
    "    output_path = f\"{file_path}/{key}_merged.tif\"\n",
    "    # Execute the command to merge rasters using subprocess and save them\n",
    "    command = [\"python\", \"glaciers_mapping_downsampled/merge_rasters.py\", \"-i\", path, \"-o\", output_path]\n",
    "    subprocess.run(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we save the RGB images in png to be able to observe them and (maybe) draw useful conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in data_paths.keys():\n",
    "    img_path = f\"{file_path}/{key}_merged\"\n",
    "    img = rio.open(f\"{img_path}.tif\").read()\n",
    "    if key[-2:] == \"gt\":\n",
    "        normalized_image = img  #(img - np.min(img)) / (np.max(img) - np.min(img))\n",
    "        normalized_image = np.squeeze(normalized_image)  # Ensure it's a single-channel image\n",
    "        rgb_image = (normalized_image * 255).astype(np.uint8)\n",
    "    else:\n",
    "        normalized_image = (img - np.min(img)) / (\n",
    "                                np.max(img) - np.min(img))\n",
    "        rgb_image = (normalized_image[:3] * 255).astype(np.uint8).transpose(1, 2, 0)\n",
    "    \n",
    "    pil_image = Image.fromarray(rgb_image)\n",
    "    save_path = f\"full_rgb_images/{key}.png\"\n",
    "    pil_image.save(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading all patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We store the patches in dictionaries : {( (position_x_of_patch, position_y_of_patch) : numpy_array_of_the_patch )}\n",
    "\n",
    "At this stage the np array of patches are of dimension (band, pixel_x, pixel_y) i.e. (4,128,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1768/1768 [00:21<00:00, 83.11it/s]\n",
      "100%|██████████| 1768/1768 [00:22<00:00, 77.95it/s]\n",
      "100%|██████████| 351/351 [00:03<00:00, 102.71it/s]\n",
      "100%|██████████| 351/351 [00:03<00:00, 109.59it/s]\n",
      "100%|██████████| 1768/1768 [00:17<00:00, 98.88it/s] \n",
      "100%|██████████| 351/351 [00:01<00:00, 187.49it/s]\n"
     ]
    }
   ],
   "source": [
    "patches_train0 = get_organized_dict_of_patches(f\"{data_paths['train_date0']}\")\n",
    "patches_train1 = get_organized_dict_of_patches(f\"{data_paths['train_date1']}\")\n",
    "patches_test0 = get_organized_dict_of_patches(f\"{data_paths['test_date0']}\")\n",
    "patches_test1 = get_organized_dict_of_patches(f\"{data_paths['test_date1']}\")\n",
    "\n",
    "patches_train_gt = get_organized_dict_of_patches(f\"{data_paths['train_gt']}\")\n",
    "patches_test_gt = get_organized_dict_of_patches(f\"{data_paths['test_gt']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_shape_pixels_by_bands(data):\n",
    "    num_dimensions = len(data.shape)\n",
    "    assert(num_dimensions == 2 or num_dimensions == 3)\n",
    "    if num_dimensions == 3:\n",
    "        num_bands = data.shape[0]\n",
    "        return data.reshape((-1, num_bands))\n",
    "    else:\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_average_feature(data):\n",
    "    # If needed convert data to the shape (num_pixels x num_bands)\n",
    "    data_2d = convert_to_shape_pixels_by_bands(data)\n",
    "    # Get the number of bands\n",
    "    num_bands = data_2d.shape[1]\n",
    "    avg_features = np.zeros(num_bands)\n",
    "    for b in range(num_bands):\n",
    "        # Compute the average value of each band (use the function np.mean)\n",
    "        avg_features[b] = np.mean(data_2d[:, b])\n",
    "    return avg_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_standard_deviation_feature(data):\n",
    "    # If needed convert data to the shape (num_pixels x num_bands)\n",
    "    data_2d = convert_to_shape_pixels_by_bands(data)\n",
    "    # Compute the standard deviation feature (using the numpy function np.std)\n",
    "    #       as in the function compute_average_feature iterate over the bands\n",
    "    #       and compute one value for each band\n",
    "    num_bands = data_2d.shape[1]\n",
    "    avg_features = np.zeros(num_bands)\n",
    "    for b in range(num_bands):\n",
    "        avg_features[b] = np.std(data_2d[:, b])\n",
    "    return avg_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_patches(data):\n",
    "    #Compute the mean, the standard deviation of the patch\n",
    "    patch = convert_to_shape_pixels_by_bands(data)\n",
    "    patch_avg = compute_average_feature(data)\n",
    "    patch_std = compute_standard_deviation_feature(data)\n",
    "    \n",
    "    #Normalize the patch\n",
    "    normalized_image = (patch - patch_avg) / patch_std\n",
    "    \n",
    "    return normalized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches_train0= {i+1: valeur for i, (_,valeur) in enumerate(patches_train0.items())}\n",
    "patches_train1= {i+1: valeur for i, (_,valeur) in enumerate(patches_train1.items())}\n",
    "patches_test0= {i+1: valeur for i, (_,valeur) in enumerate(patches_test0.items())}\n",
    "patches_test1= {i+1: valeur for i, (_,valeur) in enumerate(patches_test1.items())}\n",
    "patches_train_gt= {i+1: valeur for i, (_,valeur) in enumerate(patches_train_gt.items())}\n",
    "patches_test_gt= {i+1: valeur for i, (_,valeur) in enumerate(patches_test_gt.items())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize the train set\n",
    "patch_train0_norm={}\n",
    "patch_train1_norm={}\n",
    "for i in range(1, len(patches_train0)):\n",
    "    normalized_train0 = normalized_patches(patches_train0[i]) \n",
    "    normalized_train1 = normalized_patches(patches_train1[i]) \n",
    "    \n",
    "    patch_train0_norm[i] = normalized_train0\n",
    "    patch_train1_norm[i] = normalized_train1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize the test set\n",
    "patch_test0_norm={}\n",
    "patch_test1_norm={}\n",
    "for i in range(1, len(patches_test0)):\n",
    "    normalized_test0 = normalized_patches(patches_test0[i]) \n",
    "    normalized_test1 = normalized_patches(patches_test1[i]) \n",
    "    \n",
    "    patch_test0_norm[i] = normalized_test0\n",
    "    patch_test1_norm[i] = normalized_test1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPEOenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
